# LLM-Forge

[![npm version](https://img.shields.io/npm/v/@llm-dev-ops/llm-forge.svg?style=flat-square&color=blue)](https://www.npmjs.com/package/@llm-dev-ops/llm-forge)
[![npm downloads](https://img.shields.io/npm/dm/@llm-dev-ops/llm-forge.svg?style=flat-square&color=blue)](https://www.npmjs.com/package/@llm-dev-ops/llm-forge)
[![Tests](https://img.shields.io/badge/tests-818%20passing-brightgreen?style=flat-square)](https://github.com/globalbusinessadvisors/llm-forge/actions)
[![Coverage](https://img.shields.io/badge/coverage-93.77%25-brightgreen?style=flat-square)](https://github.com/globalbusinessadvisors/llm-forge)
[![CI](https://img.shields.io/github/actions/workflow/status/globalbusinessadvisors/llm-forge/ci.yml?style=flat-square&label=CI)](https://github.com/globalbusinessadvisors/llm-forge/actions/workflows/ci.yml)
[![Security](https://img.shields.io/github/actions/workflow/status/globalbusinessadvisors/llm-forge/security.yml?style=flat-square&label=security)](https://github.com/globalbusinessadvisors/llm-forge/actions/workflows/security.yml)
[![License](https://img.shields.io/npm/l/@llm-dev-ops/llm-forge?style=flat-square&color=blue)](LICENSE)
[![Node.js](https://img.shields.io/node/v/@llm-dev-ops/llm-forge?style=flat-square&color=blue)](https://nodejs.org)

> A unified response parser and SDK generator for LLM APIs across multiple programming languages

LLM-Forge provides a **production-ready, type-safe** way to parse and normalize responses from multiple LLM providers (OpenAI, Anthropic, Cohere, Google AI, Mistral, and more) with support for generating client libraries in **6 languages**: TypeScript, Python, Rust, Go, Java, and C#.

**Why LLM-Forge?**
- ğŸ”„ **Unified API** - One interface for 12+ LLM providers
- ğŸš€ **Ultra-Fast** - 136K-454K ops/sec parsing performance
- ğŸ›¡ï¸ **Type-Safe** - Full TypeScript inference and validation
- ğŸ§ª **Battle-Tested** - 818 tests, 93.77% coverage
- ğŸ“¦ **Zero Dependencies** - Lightweight, production-ready
- ğŸŒ **Multi-Language** - Generate SDKs in 6 languages

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Status](#-status)
- [Quick Start](#-quick-start)
- [Use Cases](#-use-cases)
- [Supported Providers](#-supported-providers)
- [Code Generation](#-code-generation)
- [Performance](#-performance)
- [Testing](#-testing)
- [Security](#-security)
- [Documentation](#-documentation)
- [Development](#-development)
- [Contributing](#-contributing)
- [Roadmap](#-roadmap)
- [Support](#-support)

---

## âœ¨ Features

### Provider Support (12 Providers)

- âœ… **Multi-Provider Parsing**: Unified response format for 12 LLM providers
- âœ… **Auto-Detection**: Automatically detect provider from response structure
- âœ… **Streaming Support**: Real-time streaming chunk parsing
- âœ… **Type-Safe**: Full TypeScript type inference and safety
- âœ… **Production Ready**: 93.77% test coverage, 818 passing tests
- âœ… **High Performance**: 136K-454K ops/sec parsing, 1-10M ops/sec detection

### Code Generation (6 Languages)

- âœ… **TypeScript**: Full type inference, decorators, async/await
- âœ… **Python**: Type hints, Pydantic models, async support
- âœ… **Rust**: Serde, strong typing, Result<T,E>
- âœ… **Java**: Record classes, Jackson, CompletableFuture
- âœ… **C#**: Record types, System.Text.Json, async streams
- âœ… **Go**: Struct tags, JSON marshaling, context support

### Enterprise Features

- âœ… **CI/CD Pipeline**: 7 GitHub Actions workflows for automation
- âœ… **Security Scanning**: Multi-layer security with CodeQL, npm audit, OSSF
- âœ… **Performance Monitoring**: Automated benchmarking and regression detection
- âœ… **Automated Releases**: npm and GitHub Packages publishing
- âœ… **Comprehensive Documentation**: Production guides and API docs

## ğŸ“Š Status

**Production Ready** âœ…

```
Test Coverage:  93.77% âœ…
Tests Passing:  818/818 (100%) âœ…
Benchmarks:     27 performance tests âœ…
CI/CD:          7 automated workflows âœ…
Documentation:  Complete âœ…
```

## ğŸš€ Quick Start

### Installation

```bash
# Using npm
npm install @llm-dev-ops/llm-forge

# Using yarn
yarn add @llm-dev-ops/llm-forge

# Using pnpm
pnpm add @llm-dev-ops/llm-forge
```

### Basic Usage - Response Parsing

```typescript
import { parseResponse } from '@llm-dev-ops/llm-forge';

// Parse any LLM provider response - automatically detects the provider
const response = await fetch('https://api.openai.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'gpt-4',
    messages: [{ role: 'user', content: 'Hello, world!' }]
  })
});

const data = await response.json();
const parsed = await parseResponse(data);

if (parsed.success) {
  console.log(parsed.response.messages[0].content);
  // Output: "Hello! How can I help you today?"

  console.log(`Provider: ${parsed.response.provider}`);
  // Output: "Provider: openai"

  console.log(`Model: ${parsed.response.model.id}`);
  // Output: "Model: gpt-4"

  console.log(`Tokens: ${parsed.response.usage.totalTokens}`);
  // Output: "Tokens: 25"
}
```

### Real-World Example - Multi-Provider Chat

```typescript
import { parseResponse } from '@llm-dev-ops/llm-forge';

async function chat(provider: 'openai' | 'anthropic' | 'cohere', message: string) {
  // Use any provider's API - LLM-Forge normalizes the response
  const endpoints = {
    openai: 'https://api.openai.com/v1/chat/completions',
    anthropic: 'https://api.anthropic.com/v1/messages',
    cohere: 'https://api.cohere.ai/v1/chat'
  };

  const response = await fetch(endpoints[provider], {
    method: 'POST',
    headers: { /* provider-specific headers */ },
    body: JSON.stringify({ /* provider-specific payload */ })
  });

  const data = await response.json();
  const parsed = await parseResponse(data);

  if (parsed.success) {
    return {
      text: parsed.response.messages[0].content,
      tokens: parsed.response.usage.totalTokens,
      cost: calculateCost(parsed.response.usage, parsed.response.model.id)
    };
  }

  throw new Error(parsed.error.message);
}

// Works seamlessly with any provider
const openAIResult = await chat('openai', 'Explain quantum computing');
const claudeResult = await chat('anthropic', 'Explain quantum computing');
const cohereResult = await chat('cohere', 'Explain quantum computing');
```

### Auto-Detection

```typescript
import { parseResponse } from '@llm-dev-ops/llm-forge';

// Automatically detects provider from response structure
const openAIResponse = await parseResponse(openAIData);    // Detects OpenAI
const anthropicResponse = await parseResponse(claudeData); // Detects Anthropic
const cohereResponse = await parseResponse(cohereData);    // Detects Cohere
```

## ğŸ’¡ Use Cases

### 1. Multi-Provider Chatbots
Build chatbots that can seamlessly switch between different LLM providers based on cost, performance, or availability.

### 2. LLM Abstraction Layer
Create a unified interface for your application that works with any LLM provider, making it easy to switch providers or A/B test different models.

### 3. Cost Optimization
Parse usage data from multiple providers to track and optimize token usage and costs across your organization.

### 4. SDK Generation
Generate type-safe client libraries in multiple programming languages from OpenAPI specifications, accelerating development across teams.

### 5. LLM Gateway/Proxy
Build an LLM gateway that routes requests to different providers, normalizes responses, and provides unified monitoring and analytics.

### 6. Testing & Development
Easily switch between providers during development and testing without changing your application code.

### Provider-Specific Parsing

```typescript
import { OpenAIProvider, AnthropicProvider } from '@llm-dev-ops/llm-forge';

const openai = new OpenAIProvider();
const result = await openai.parse(openAIResponse);

const anthropic = new AnthropicProvider();
const claudeResult = await anthropic.parse(anthropicResponse);
```

### Streaming Support

```typescript
import { OpenAIProvider } from '@llm-dev-ops/llm-forge';

const provider = new OpenAIProvider();

// Parse streaming chunks
for await (const chunk of streamingResponse) {
  const parsed = await provider.parseStream(chunk);
  if (parsed.success) {
    process.stdout.write(parsed.response.messages[0].content);
  }
}
```

## ğŸ¯ Supported Providers

| Provider | Status | Detection | Parsing | Streaming |
|----------|--------|-----------|---------|-----------|
| **OpenAI** | âœ… Complete | âœ… | âœ… | âœ… |
| **Anthropic** | âœ… Complete | âœ… | âœ… | âœ… |
| **Google AI** | âœ… Complete | âœ… | âœ… | âœ… |
| **Cohere** | âœ… Complete | âœ… | âœ… | âœ… |
| **Mistral** | âœ… Complete | âœ… | âœ… | âœ… |
| **Azure OpenAI** | âœ… Complete | âœ… | âœ… | âœ… |
| **Hugging Face** | âœ… Complete | âœ… | âœ… | âš ï¸ Limited |
| **Replicate** | âœ… Complete | âœ… | âœ… | âš ï¸ Limited |
| **Together AI** | âœ… Complete | âœ… | âœ… | âš ï¸ Limited |
| **Perplexity** | âœ… Complete | âœ… | âœ… | âœ… |
| **OpenRouter** | âœ… Complete | âœ… | âœ… | âœ… |
| **Custom** | âœ… Complete | âœ… | âœ… | âš ï¸ Provider-dependent |

## ğŸ”§ Code Generation

### Generate TypeScript Client

```typescript
import { generateTypeScript } from '@llm-dev-ops/llm-forge';

const schema = {
  name: 'ChatCompletion',
  properties: {
    messages: { type: 'array', items: { type: 'Message' } },
    model: { type: 'string' }
  }
};

const code = await generateTypeScript(schema);
console.log(code);
```

### Supported Languages

| Language | Status | Package Manager | Type Safety | Async Support |
|----------|--------|----------------|-------------|---------------|
| **TypeScript** | âœ… Complete | npm | Full | async/await |
| **Python** | âœ… Complete | pip | Type hints | async/await |
| **Rust** | âœ… Complete | cargo | Strong | tokio |
| **Java** | âœ… Complete | Maven/Gradle | Strong | CompletableFuture |
| **C#** | âœ… Complete | NuGet | Strong | async/await |
| **Go** | âœ… Complete | go modules | Static | goroutines |

## ğŸ“ˆ Performance

### Benchmarks (ops/sec)

**Provider Detection:**
- OpenAI: 9.7M ops/sec
- Anthropic: 9.4M ops/sec
- Cohere: 8.7M ops/sec
- Mistral: 6.7M ops/sec
- Google AI: 5.5M ops/sec

**Response Parsing:**
- Mistral: 454K ops/sec (fastest)
- OpenAI: 422K ops/sec
- Anthropic: 368K ops/sec
- Cohere: 313K ops/sec
- Google AI: 137K ops/sec

**Streaming:**
- OpenAI: 504K chunks/sec
- Anthropic: 485K chunks/sec

*Benchmarked on Node.js 20 with Vitest bench suite (27 benchmarks)*

## ğŸ—ï¸ Architecture

LLM-Forge uses a layered architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Provider Responses (OpenAI, Anthropic, etc.)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Provider Detection & Auto-detection                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Unified Response Parsing                           â”‚
â”‚  - Message extraction                               â”‚
â”‚  - Metadata normalization                           â”‚
â”‚  - Token usage tracking                             â”‚
â”‚  - Error handling                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Code Generation (6 languages)                      â”‚
â”‚  - Type generation                                  â”‚
â”‚  - Client generation                                â”‚
â”‚  - Serialization                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for detailed architecture documentation.

## ğŸ§ª Testing

### Test Coverage

```
Overall Coverage:    93.77%
Providers Coverage:  92.68%
Generators Coverage: 98.17%
Parsers Coverage:    98.04%
Core Coverage:       97.73%

Total Tests: 818 passing
Test Files:  26 files
Duration:    ~12 seconds
```

### Run Tests

```bash
# Run all tests
npm test

# Run with coverage
npm run test:coverage

# Run benchmarks
npm run bench

# Run specific test file
npm test tests/providers/integration.test.ts
```

## ğŸ”’ Security

LLM-Forge implements multiple security layers:

- âœ… **Daily Security Scans**: Automated vulnerability detection
- âœ… **CodeQL Analysis**: Static security analysis
- âœ… **Secret Detection**: TruffleHog scanning
- âœ… **License Compliance**: Automated license checking
- âœ… **Dependency Updates**: Dependabot automation
- âœ… **OSSF Scorecard**: Security best practices validation

See [docs/CI_CD_PIPELINE.md](docs/CI_CD_PIPELINE.md) for security documentation.

## ğŸ”„ CI/CD Pipeline

7 automated workflows ensure quality:

1. **PR Validation** - Quality gates for pull requests
2. **Continuous Integration** - Multi-OS testing (Ubuntu, macOS, Windows)
3. **Security Scanning** - Multi-layer security analysis
4. **Performance Monitoring** - Benchmark tracking and regression detection
5. **Release & Publish** - Automated npm publishing
6. **Dependabot Auto-Merge** - Safe dependency updates
7. **Stale Management** - Issue/PR lifecycle management

See [.github/README.md](.github/README.md) for workflow documentation.

## ğŸ“š Documentation

### User Guides
- [Production Readiness](docs/PRODUCTION_READINESS.md) - Deployment guide
- [CI/CD Pipeline](docs/CI_CD_PIPELINE.md) - Pipeline documentation
- [Architecture](docs/ARCHITECTURE.md) - System architecture

### Implementation
- [Provider System](docs/provider-system-implementation.md) - Provider implementation
- [Template Engine](docs/template-engine.md) - Code generation templates
- [Language Strategy](docs/language-strategy.md) - Multi-language support

### Reference
- [Implementation Summary](docs/IMPLEMENTATION_COMPLETE.md) - Complete implementation details
- [DevOps Deliverables](docs/DEVOPS_DELIVERABLES.md) - CI/CD and deployment details
- [Multi-Language Success](docs/MULTI-LANGUAGE-SUCCESS.md) - Code generation implementation

## ğŸ› ï¸ Development

### Prerequisites

- Node.js >= 20.0.0
- npm >= 10.0.0
- TypeScript >= 5.3.3

### Setup

```bash
# Clone repository
git clone https://github.com/globalbusinessadvisors/llm-forge.git
cd llm-forge

# Install dependencies
npm install

# Run tests
npm test

# Build project
npm run build

# Run development mode
npm run dev

# Run benchmarks
npm run bench

# Run all quality checks
npm run quality
```

### CLI Usage

LLM-Forge includes a command-line interface for generating SDKs:

```bash
# Run CLI in development
npm run cli -- --help

# Generate SDK from OpenAPI spec
npm run cli -- generate --input openapi.json --output ./sdk --language typescript

# After installation (globally or in project)
npx llm-forge generate --input openapi.json --language python
```

### Project Structure

```
llm-forge/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/           # Template engine and type system
â”‚   â”œâ”€â”€ generators/     # Language-specific code generators
â”‚   â”œâ”€â”€ parsers/        # OpenAPI and Anthropic parsers
â”‚   â”œâ”€â”€ providers/      # Provider-specific parsers (12 providers)
â”‚   â”œâ”€â”€ schema/         # Schema validation
â”‚   â””â”€â”€ types/          # TypeScript type definitions
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ core/           # Core functionality tests
â”‚   â”œâ”€â”€ generators/     # Code generator tests
â”‚   â”œâ”€â”€ parsers/        # Parser tests
â”‚   â”œâ”€â”€ providers/      # Provider tests (integration, benchmarks)
â”‚   â””â”€â”€ schema/         # Schema validation tests
â”œâ”€â”€ docs/               # Comprehensive documentation
â”œâ”€â”€ examples/           # Example usage
â”œâ”€â”€ scripts/            # Build and utility scripts
â””â”€â”€ .github/
    â”œâ”€â”€ workflows/      # 7 CI/CD workflows
    â””â”€â”€ dependabot.yml  # Dependency automation
```

### Available Scripts

```bash
npm test              # Run all tests
npm run test:coverage # Run tests with coverage report
npm run bench         # Run performance benchmarks
npm run type-check    # TypeScript type checking
npm run lint          # ESLint code linting
npm run format        # Prettier code formatting
npm run build         # Build package
npm run clean         # Clean build artifacts
npm run quality       # Run all quality checks
```

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes with tests
4. Run quality checks (`npm run quality`)
5. Commit your changes (`git commit -m 'feat: add amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

All PRs must pass:
- âœ… TypeScript type checking
- âœ… ESLint linting
- âœ… Prettier formatting
- âœ… All 818 tests
- âœ… 93%+ code coverage
- âœ… Security scans

See [docs/CI_CD_PIPELINE.md](docs/CI_CD_PIPELINE.md) for detailed contribution guidelines.

## ğŸ“¦ Publishing

### npm

```bash
npm install @llm-dev-ops/llm-forge
```

### GitHub Packages

```bash
npm install @llm-dev-ops/llm-forge
```

## ğŸ—ºï¸ Roadmap

### âœ… Phase 1: Foundation (Complete)
- âœ… Provider response parsing (12 providers)
- âœ… Unified response format
- âœ… Auto-detection system
- âœ… Streaming support

### âœ… Phase 2: Code Generation (Complete)
- âœ… TypeScript generator
- âœ… Python generator
- âœ… Rust generator
- âœ… Java generator
- âœ… C# generator
- âœ… Go generator

### âœ… Phase 3: Production Ready (Complete)
- âœ… Comprehensive testing (818 tests)
- âœ… 93.77% code coverage
- âœ… Performance benchmarking
- âœ… CI/CD pipeline (7 workflows)
- âœ… Security scanning
- âœ… Complete documentation

### ğŸ”® Phase 4: Future Enhancements (Planned)
- [ ] CLI tool for SDK generation
- [ ] Plugin system for custom providers
- [ ] Cost tracking and analytics
- [ ] Advanced observability
- [ ] Custom provider templates
- [ ] GraphQL support

## ğŸ“„ License

Apache License 2.0 - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

Built with enterprise-grade quality using:
- **Testing**: Vitest
- **CI/CD**: GitHub Actions
- **Security**: CodeQL, TruffleHug, OSSF Scorecard
- **Coverage**: Codecov
- **Type Safety**: TypeScript

---

## ğŸ“Š Project Metrics

```
Lines of Code:       ~15,000
Test Coverage:       93.77%
Tests:              818 passing
Benchmarks:         27 performance tests
Providers:          12 supported
Languages:          6 code generators
CI/CD Workflows:    7 automated
Documentation:      35+ comprehensive docs
Performance:        136K-454K ops/sec parsing
Security:           Multi-layer scanning
```

## ğŸ†˜ Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/globalbusinessadvisors/llm-forge/issues)
- **Discussions**: [GitHub Discussions](https://github.com/globalbusinessadvisors/llm-forge/discussions)
- **CI/CD Status**: [GitHub Actions](https://github.com/globalbusinessadvisors/llm-forge/actions)
- **npm Package**: [npmjs.com](https://www.npmjs.com/package/@llm-dev-ops/llm-forge)
- **Coverage**: [Codecov](https://codecov.io/gh/globalbusinessadvisors/llm-forge)

---

<div align="center">

**Status**: âœ… Production Ready | **License**: Apache 2.0 | **Version**: 1.0.0

Made with â¤ï¸ by the LLM-Dev-Ops Team

[Getting Started](#-quick-start) Â· [Documentation](docs/) Â· [Report Bug](https://github.com/globalbusinessadvisors/llm-forge/issues) Â· [Request Feature](https://github.com/globalbusinessadvisors/llm-forge/issues/new)

</div>
